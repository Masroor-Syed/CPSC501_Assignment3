{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cpsc501_Q1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa-6PpVw3_GC",
        "colab_type": "text"
      },
      "source": [
        "Masroor Hussain Syed<br>\n",
        "30023900\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnOpjOuS4FNc",
        "colab_type": "text"
      },
      "source": [
        "Load tensorflow version 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4eZDEIAx0aH",
        "colab_type": "code",
        "outputId": "03de7fd9-e90c-4e76-db11-978a116d7d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92NcWA7T4Jlg",
        "colab_type": "text"
      },
      "source": [
        "# Loading and Preprocessing \n",
        "\n",
        "1. I load the tensor library,  MNIST data and split it into the training set (x_train and y_train) and validation set (x_test and y_test). This is very important because we must test our models with data it hasn't seen before, otherwise, our model will overfit on the training data and will not perform well with real-world data. \n",
        "2. After loading the data, I normalize it to values between 0 and 1 because it makes training the neural network faster because it leads to faster convergence of the network.\n",
        "\n",
        "# The Model\n",
        "1. I used a Sequential DNN model for this problem because it is the standard and most neural networks for this dataset use it.\n",
        "2. The model consists of an input layer with 784 neurons and 2 hidden layers each with 256 neurons and an output layer with 10 neurons.\n",
        "3. I used 'relu' as the activation function for my hidden layers because it is less computationally intense, converges faster because It doesnâ€™t have the vanishing gradient problem and it doesn't activate for negative values so it is sparsely activated.\n",
        "4. The output layer using the softmax function as its activation function because it returns the probability distribution of the prediction for all possible categories, therefore it is great for classification networks.\n",
        "\n",
        "# Model Compilation and Training \n",
        "\n",
        "1. I used 'Adam' as the optimizer for my model because it was providing the best result after experimenting with 'sgd' and other optimizers.\n",
        "2. I used sparse_categorical_crossentropy as the loss function because it is great for classification networks and provided the best results after experimentation. \n",
        "3. To train the network I used 'accuracy' as metric because it is the standard practice for non-binary classifiers like this.\n",
        "4. I trained my model using 20 epocs because the training plateaus here and the networks begin to overfit on this dataset.\n",
        "\n",
        "# Testing, Evaluating and Saving \n",
        "\n",
        "1. After training the model, I test it by running the model on the validation dataset and measure its accuracy. The model is able to predict 98% of the training data accurately as required.\n",
        "2.I then save the trained model in the final MNIST.h5\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITCcIDcJx02Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(\"--Get data--\")\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"--Process data--\")\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "print(\"--Make model--\")\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"--Fit model--\")\n",
        "model.fit(x_train, y_train, epochs=20, verbose=1)\n",
        "\n",
        "print(\"--Evaluate model--\")\n",
        "model_loss, model_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print(f\"Model Loss:    {model_loss:.2f}\")\n",
        "print(f\"Model Accuray: {model_acc*100:.1f}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqKuNbSJEaWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save Model to MNIST.h5\n",
        "model.save('MNIST.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vc4ePZwERJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download('MNIST.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}